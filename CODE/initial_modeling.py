# -*- coding: utf-8 -*-
"""Modeling

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1om7AOvqiW8pFNpfGIgeP_0CyIniKPN_z
"""

import numpy as np
import pandas as pd

from google.colab import drive
drive.mount("/content/gdrive")
df = pd.read_csv("/content/gdrive/My Drive/df_kaggle_mod_embed.csv")

df.drop("Unnamed: 0", axis = 1, inplace = True)
df.drop("TEXT", axis = 1, inplace = True)

df = df.rename({'EMBEDDINGS': 'embeddings', 'LABEL': 'label'}, axis=1)
df = df[["embeddings","label"]]

df

def clean_embeddings(row):
  data = row["embeddings"]
  data = data.replace('[',"")
  data = data.replace(']',"")
  list_of_nums = data.split(" ")

  clean_list = []
  final_clean_list = []

  for entry in list_of_nums:
    if (entry != ''):
      clean_list.append(entry)

  final_clean_list = [float(entry) for entry in clean_list]

  return final_clean_list

df["clean_embeddings"] = df.apply(lambda x: clean_embeddings(x), axis = 1)

df.drop("embeddings", axis=1, inplace = True)
df = df.rename({"clean_embeddings" : "embeddings"}, axis=1)
df = df [["embeddings","label"]]
df_embeddings_explode = pd.DataFrame(df["embeddings"].to_list())
df.drop("embeddings", axis=1, inplace = True)
df_embeddings_explode.fillna(0, inplace = True)

df_final = pd.concat([df_embeddings_explode, df], axis = 1)
def num_label(row):
  label = row['label']
  if (label == "fake"):
    return 1
  else:
    return 0
    
df_final['label'] = df_final.apply(lambda x: num_label(x), axis = 1)

df_final

df_final.to_csv('df_clean_embeddings.csv')
!cp df_clean_embeddings.csv "/content/gdrive/My Drive/"

import matplotlib.pyplot as plt 
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, f1_score
import joblib

X = df_final.drop("label", axis = 1)
y = df_final["label"]

data_size = 21000
train_split = 0.7
sup_val_split = 0.1
unsup_val_split = 0.1
test_val_split = 0.1

X_train, X_test_1, y_train, y_test_1 = train_test_split(df_final.drop('label', axis=1), df_final['label'], test_size=(1-train_split), random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_test_1, y_test_1, test_size=0.333, random_state=42)
X_val_sup, X_val_unsup, y_val_sup, y_val_unsup = train_test_split(X_val, y_val, test_size=0.5, random_state=42)

param_grid = { 
    'C' : [2, 6, 10]
}
LogReg = GridSearchCV(estimator=LogisticRegression(solver='liblinear',random_state=0), param_grid=param_grid)
LogReg.fit(X_train, y_train)   
print("Best Params: {}".format(LogReg.best_params_))
print("Best Scores: {}".format(LogReg.best_score_))   

pred = LogReg.predict(X_val_sup)
accuracy = LogReg.score(X_val_sup, y_val_sup)
print("Accuracy = {}".format(accuracy))
print("Macro F1 Score = {}".format(f1_score(y_val_sup, pred, average='macro')))
print(classification_report(y_val_sup, pred))

LogReg = LogisticRegression(solver='liblinear', C=2, random_state=0)
LogReg.fit(X_train, y_train)   

pred = LogReg.predict(X_val_unsup)
accuracy = LogReg.score(X_val_unsup, y_val_unsup)
print("Accuracy = {}".format(accuracy))
print("Macro F1 Score = {}".format(f1_score(y_val_unsup, pred, average='macro')))
print(classification_report(y_val_unsup, pred))

filename = "/content/gdrive/My Drive/models/LogReg.pkl"
joblib.dump(LogReg, filename)

param_grid = { 
    'n_estimators': [16, 256],
    'max_depth' : [8, 16]
}
RFModel = GridSearchCV(estimator=RandomForestClassifier(random_state=0), param_grid=param_grid)
RFModel.fit(X_train, y_train)   
print("Best Params: {}".format(RFModel.best_params_))
print("Best Scores: {}".format(RFModel.best_score_))  

pred = RFModel.predict(X_val_sup)
accuracy = RFModel.score(X_val_sup, y_val_sup)
print("Accuracy = {}".format(accuracy))
print("Macro F1 Score = {}".format(f1_score(y_val_sup, pred, average='macro')))
print(classification_report(y_val_sup, pred))

param_grid = { 
    'var_smoothing': np.logspace(0,-9, num=100)
}
NBModel = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid)
NBModel.fit(X_train, y_train)   
print("Best Params: {}".format(NBModel.best_params_))
print("Best Scores: {}".format(NBModel.best_score_))  

pred = NBModel.predict(X_val_sup)
accuracy = NBModel.score(X_val_sup, y_val_sup)
print("Accuracy = {}".format(accuracy))
print("Macro F1 Score = {}".format(f1_score(y_val_sup, pred, average='macro')))
print(classification_report(y_val_sup, pred))

from sklearn.neural_network import MLPClassifier
MLPModel = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1, max_iter = 5000)
MLPModel.fit(X_train, y_train)
pred = MLPModel.predict(X_val_sup)
accuracy = MLPModel.score(X_val_sup, y_val_sup)
print("Accuracy = {}".format(accuracy))
print(classification_report(y_val_sup, pred))

prob = pd.DataFrame(LogReg.predict_proba(X_test))
prob['max'] = prob.apply(lambda x: max(x[0],x[1]), axis=1)
prob[prob['max'] >= 0.65]